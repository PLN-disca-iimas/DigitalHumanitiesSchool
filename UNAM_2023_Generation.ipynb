{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# UNAM Digital Humanities Summer School\n",
        "# Pratical course: generating text (summarization)\n",
        "\n",
        "# September 2023\n",
        "\n",
        "Benjamin Piwowarski (benjamin.piwowarski@cnrs.fr)  -- MLIA/ISIR, Sorbonne Universit√©\n",
        "\n",
        "In this notebook, we'll look at a (conditional) text generation task, that is, we want to generate a text in response to an input (a document to summarize, a question of a user, etc.)\n",
        "\n",
        "We'll also look at how to use the huggingface trainer API to simplify writing the learning loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHHuqToUv3Ka"
      },
      "source": [
        "## Setting up the environnement\n",
        "\n",
        "The following cells create the appropriate python environnement, import important modules, and define a few useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5uLu28av3Kb"
      },
      "outputs": [],
      "source": [
        "# Installing the packages\n",
        "\n",
        "try:\n",
        "    from easypip import easyimport, easyinstall, is_notebook\n",
        "except ModuleNotFoundError as e:\n",
        "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
        "    from easypip import easyimport, easyinstall, is_notebook\n",
        "\n",
        "easyinstall(\"torch\")\n",
        "easyinstall(\"numpy\")\n",
        "easyinstall(\"pandas\")\n",
        "easyinstall(\"nltk\")\n",
        "easyinstall(\"rouge_score\")\n",
        "easyinstall(\"datasets\")\n",
        "easyinstall(\"accelerate\")\n",
        "easyinstall(\"transformers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F7t2P6_v3Kc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from datasets import load_dataset, load_metric\n",
        "import datasets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ-sATqAv3Kd"
      },
      "outputs": [],
      "source": [
        "# Define the device on which to run the model\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "\n",
        "print(\"Selected device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZdvcITCv3Kd"
      },
      "outputs": [],
      "source": [
        "def show_random_elements(dataset, num_examples=10):\n",
        "    \"\"\"Shows a subset of a pandas dataset\"\"\"\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWjVZbvbv3Kd"
      },
      "source": [
        "In the following, we are going to use the [T5](https://arxiv.org/abs/1910.10683) model, which is a Transformer pre-trained on a set of tasks\n",
        "in *seq2seq* mode. In particular, we use here the [\"small\" version](https://huggingface.co/t5-small) from Huggingface.\n",
        "\n",
        "We use `AutoModelForSeq2SeqLM` to use encoder-decoder models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTNPq4M_v3Ke"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model_checkpoint = \"t5-small\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We use the [Datasets library](https://github.com/huggingface/datasets) provided by huggingface. This library provides access to a large number of text resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "# To speed up processing, we only use 10% of the train set, and 10% of the validation one (5% for validation, 5% for test)\n",
        "raw_datasets = load_dataset(\"xsum\", split={\"train\": \"train[:10%]\", \"validation\": \"validation[:5%]\", \"test\": \"validation[5%:10%]\"})\n",
        "metric = load_metric(\"rouge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "The `dataset` object is a [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains a set of datasets used for training, validation and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWiVUF0jIrIv"
      },
      "outputs": [],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "# Dataset extract\n",
        "\n",
        "Here's a random extract from the dataset, with a document (to be summarized) and the expected summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZy5tRB_IrI7"
      },
      "outputs": [],
      "source": [
        "show_random_elements(raw_datasets[\"train\"], 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnjDIuQ3IrI-"
      },
      "source": [
        "The metric is [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o4rUteaIrI_"
      },
      "outputs": [],
      "source": [
        "metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "We can use the `compute` method to calculate a metric on a set of text pairs.\n",
        "\n",
        "ROUGE metrics are statistics on the number of n-grams (bigrams for ROUGE-2, etc.). See this [blog](https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460) for a fuller description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XN1Rq0aIrJC"
      },
      "outputs": [],
      "source": [
        "predictions = [\"I was going to the park\"]\n",
        "expected = [\"I was hiking\"]\n",
        "metric.compute(predictions=predictions, references=expected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "We're going to use the tokenizer corresponding to the pre-trained Transformer model `T5-small`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZHwgnNZv3Ki"
      },
      "source": [
        "To pre-process the data, we add \"summarize: \" to each text we need to summarize (this is how `T5` was trained). We therefore define a pre-processing function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJuNcFszv3Ki"
      },
      "outputs": [],
      "source": [
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"summarize: \" + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "An example of use is shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b70jh26IrJS"
      },
      "outputs": [],
      "source": [
        "for key, value in preprocess_function(raw_datasets['train'][:2]).items():\n",
        "    print(key, value[0][:10], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "The dataset can be pre-processed to speed up further processing - using the `map` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLcJcVigv3Kk"
      },
      "source": [
        "# Using a seq2seq model\n",
        "\n",
        "The Transformer library makes it very easy to use a Seq2Seq model, using the `generate` method ([documentation](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate)). We explore the various possibilities next.\n",
        "\n",
        "We first define a `summarize` function that generate text by encoding the sentence to summarize with a prefix (by default, `summarize: `."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN-zd1h0v3Kk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def summarize(model, sentence, prefix=\"summarize: \", **args):\n",
        "    encoder_inputs = tokenizer(f\"{prefix}{sentence}\", return_tensors=\"pt\")\n",
        "    output = model.generate(encoder_inputs[\"input_ids\"].to(device),  **args)\n",
        "    return tokenizer.batch_decode(output, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQAlcl-rv3Kk"
      },
      "outputs": [],
      "source": [
        "sentence = \"Mary went to the zoo yesterday with her father. She saw plenty of animals. The one she prefered was a tiger, but she also enjoyed the monkeys.\"\n",
        "\n",
        "display(HTML(f\"Sentence to summarize: <b>{sentence}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BScmMu5v3Kl"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "The simplest method is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YHaNmvrv3Kl"
      },
      "outputs": [],
      "source": [
        "summarize(model, sentence, num_return_sequences=10, max_length=50, output_scores=True, do_sample=True, early_stopping=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia5-8oyjv3Kl"
      },
      "source": [
        "## Nucleus sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdIW0Bygv3Kl"
      },
      "outputs": [],
      "source": [
        "summarize(model, sentence, num_return_sequences=10, max_length=50, top_p=10, output_scores=True, do_sample=True, early_stopping=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMI5evfNv3Kl"
      },
      "source": [
        "## Top-K sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5UpLv_Xv3Kl"
      },
      "outputs": [],
      "source": [
        "summarize(model, sentence, num_return_sequences=10, max_length=50, do_sample=True, top_k=10, early_stopping=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbk3pnApv3Km"
      },
      "source": [
        "## Beam search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NLL3ZUvv3Km"
      },
      "outputs": [],
      "source": [
        "# Beam search\n",
        "\n",
        "display(summarize(model, sentence, max_length=50, num_beams=5, num_return_sequences=5, early_stopping=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Finetuning T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "As you can see, the model doesn't work all that well directly. We therefore need to fine-tune its parameters in order to obtain better-quality summaries. To do this, we'll use the `Seq2SeqTrainer` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "The first step is to define the arguments via the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments) class. The first argument is the name of the folder that will contain the *checkpoints*. There are plenty of other arguments that can control learning, but the main ones are given below (at least, in the context of this notebook!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoTokenizer\n",
        "\n",
        "gradient_accumulation_steps = 4 if device.type == \"cuda\" else 1\n",
        "eval_steps = 25\n",
        "max_steps = 100\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    \"xp/summarization\",\n",
        "\n",
        "    # We evaluate every n steps\n",
        "    evaluation_strategy = \"steps\",\n",
        "    eval_steps=eval_steps * gradient_accumulation_steps,\n",
        "\n",
        "    # 200 learning steps\n",
        "    max_steps=max_steps * gradient_accumulation_steps,\n",
        "\n",
        "    # Optimizer settings\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=25,\n",
        "\n",
        "    # Batch size\n",
        "    per_device_train_batch_size=64 // gradient_accumulation_steps,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    per_device_eval_batch_size=32,\n",
        "\n",
        "    # Logging\n",
        "    logging_dir=\"xp/summarization/runs\",\n",
        "    logging_steps=.1, # log 10 times\n",
        "    logging_strategy=\"steps\",\n",
        "\n",
        "    # 3 checkpoints maximum\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Save the model every 25 steps\n",
        "    save_steps=eval_steps * gradient_accumulation_steps,\n",
        "\n",
        "    # Generate to evaluate\n",
        "    predict_with_generate=True,\n",
        "\n",
        "    # Speed up training using FP16 (floats with 16 bits)\n",
        "    # only CUDA\n",
        "    fp16=device.type == \"cuda\",\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "To prepare the data, specific pre-processing is required (padding of inputs as well as outputs) - the [DataCollatorForSeq2Seq](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq) class is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o0euBYHv3Kn"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "Next, you need to specify how the `Seq2SeqTrainer` metrics are to be calculated from the predictions. We will use the RED metrics defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # We replace -100 by [PAD] so we can decode\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Adds \"\\n\" after each sentence (1 sentence per line for nltk)\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    # Computes the metric\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    # Results (x 100)\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "    # Adds mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Lastly, a `Seq2SeqTrainer` is used to start the learning process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We setup tensorboard to follow the learning process"
      ],
      "metadata": {
        "id": "Surg09NHYZWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir xp/summarization/runs"
      ],
      "metadata": {
        "id": "Hw-mA1TmYdEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "Now just use the `train` method - and wait (about 5 minutes if you haven't changed the parameters)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "model.to(device); # just in case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4a-imX9v3Ko"
      },
      "source": [
        "# Using the model\n",
        "\n",
        "We can now look at the results after learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAcLAe1lv3Ko"
      },
      "outputs": [],
      "source": [
        "display(summarize(model.to(device), sentence, max_length=50, num_beams=5, num_return_sequences=5, early_stopping=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxTiQqp1v3Ko"
      },
      "source": [
        "Note that the model has been saved and can be loaded afterwards to keep the training information with\n",
        "\n",
        "```py\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"xp/summarization/checkpoint-10\").to(device)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqkMAM4Mv3Ko"
      },
      "source": [
        "# Gooing deeper\n",
        "\n",
        "The objective of this section is to go a bit deeper in looking at how a transformer model like T5 generates text. Instead of using the `generate` model, we are going to use the model at a lower level - and step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUzGrQaO9Xf5"
      },
      "source": [
        "## 1. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0PKLWrHv3Kp"
      },
      "outputs": [],
      "source": [
        "tokenized = tokenizer(f\"summarize: {sentence}\", return_tensors=\"pt\")\n",
        "\n",
        "# Show how the text as been converted\n",
        "\" / \".join(tokenizer.convert_ids_to_tokens(tokenized.input_ids[0])[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsSPYDvR9a4U"
      },
      "source": [
        "## 2. Now, we encode the input using the model encoder\n",
        "\n",
        "The encoder takes a sequence of $n$ tokens, and computes their contextual representation in $\\mathbb{R}^d$, given into the `last_hidden_state` of the output of the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHwDTI6Zv3Kp"
      },
      "outputs": [],
      "source": [
        "out_encoder=model.encoder(**tokenized.to(device))\n",
        "\n",
        "# The output is 1 x n x d\n",
        "out_encoder.last_hidden_state.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLuX22yN-av3"
      },
      "source": [
        "## 3. Generating the output\n",
        "\n",
        "We need to generate the first token: use the special token `<pad>`. The then use the decoder on this sequence, condtionned on the input - here, the input. We obtain a representation of the decoder output for `<pad>` which we project onto the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy96-fQev3Kp"
      },
      "outputs": [],
      "source": [
        "output = tokenizer(\"<pad>\", return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "decoder_output = model.decoder(**output, encoder_hidden_states=out_encoder.last_hidden_state)\n",
        "\n",
        "def get_top(decoder_output, k=10, do_print=True):\n",
        "    all_logits = model.lm_head(decoder_output.last_hidden_state)[-1]\n",
        "    top_logits, top_token_ids = all_logits[-1].topk(10)\n",
        "    if do_print:\n",
        "        print(\", \".join([f\"p({k}) = {v:.3f}\" for k, v in zip(tokenizer.convert_ids_to_tokens(top_token_ids), top_logits.softmax(0))]))\n",
        "    return top_logits, top_token_ids\n",
        "\n",
        "get_top(decoder_output);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we do use the greedy decoding strategy to generate the summarized sentence. Feel free to implement others, like sampling, nucleus sampling or beam search!"
      ],
      "metadata": {
        "id": "KAhfDQDAXP-0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VfaUqLUv3Kp"
      },
      "outputs": [],
      "source": [
        "# For reference\n",
        "summarize(model.to(device), sentence, max_length=50, num_beams=1,  early_stopping=True)\n",
        "\n",
        "# Loop and construct the sentence step by step\n",
        "output_ids = tokenizer(\"<pad>\", return_tensors=\"pt\", add_special_tokens=False).to(device).input_ids\n",
        "\n",
        "for ix in range(20):\n",
        "    print(tokenizer.convert_ids_to_tokens(output_ids[0]), \" -> \", tokenizer.decode(output_ids[0]))\n",
        "\n",
        "    decoder_output = model.decoder(input_ids=output_ids, encoder_hidden_states=out_encoder.last_hidden_state)\n",
        "    top_logits, top_token_ids = get_top(decoder_output)\n",
        "\n",
        "    output_ids = torch.cat((output_ids, torch.LongTensor([[top_token_ids[0]]]).to(device)), dim=1)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4D7pW9lBvXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "e6ec78a1532721e0d9a916b62159416ed1d339b4bc643ca2e942bc779b0abb0f"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}